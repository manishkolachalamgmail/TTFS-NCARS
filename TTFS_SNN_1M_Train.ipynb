{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e3a9e9-2f39-472b-8ae7-b87011fb12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0f9ca2-ab72-4204-8379-99b9d79b9acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading N-Cars dataset...\n",
      "Train sequences: 15423\n",
      "Test sequences: 8606\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Load N-Cars dataset\n",
    "DATASET_PATH = Path(\"ncars_dataset.pkl\")\n",
    "\n",
    "print(\"Loading N-Cars dataset...\")\n",
    "with open(DATASET_PATH, \"rb\") as f:\n",
    "    ncars_data = pickle.load(f)\n",
    "\n",
    "print(f\"Train sequences: {len(ncars_data['train'])}\")\n",
    "print(f\"Test sequences: {len(ncars_data['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4107c0-5a33-4c70-bb4e-506882f94a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset: 15423 samples\n",
      "Test dataset: 8606 samples\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset for N-Cars\n",
    "class NCarsDatasetForTTFS(Dataset):\n",
    "    def __init__(self, sequences, num_time_bins=10, height=100, width=120):\n",
    "        self.sequences = sequences\n",
    "        self.num_time_bins = num_time_bins\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        xs, ys, ts, ps = seq['x'], seq['y'], seq['t'], seq['p']\n",
    "        label = seq['label']\n",
    "        \n",
    "        # Normalize timestamps\n",
    "        if len(ts) > 0:\n",
    "            t_min, t_max = ts.min(), ts.max()\n",
    "            ts_norm = (ts - t_min) / (t_max - t_min) if t_max > t_min else np.zeros_like(ts)\n",
    "        else:\n",
    "            ts_norm = np.array([])\n",
    "        \n",
    "        # Create tensor in format [C=2, X=120, Y=100, T=10]\n",
    "        ttfs_tensor = np.zeros((2, self.width, self.height, self.num_time_bins), dtype=np.float32)\n",
    "        \n",
    "        # Fill events\n",
    "        if len(xs) > 0:\n",
    "            valid = (xs >= 0) & (xs < self.width) & (ys >= 0) & (ys < self.height)\n",
    "            xs_v, ys_v, ts_v, ps_v = xs[valid], ys[valid], ts_norm[valid], ps[valid]\n",
    "            \n",
    "            t_bins = np.clip((ts_v * (self.num_time_bins - 1)).astype(np.int32), 0, self.num_time_bins - 1)\n",
    "            spike_vals = np.ones_like(ts_v)  # Unit spike magnitude\n",
    "            \n",
    "            for x, y, t_bin, p, val in zip(xs_v, ys_v, t_bins, ps_v, spike_vals):\n",
    "                pol_ch = 1 if p == 1 else 0\n",
    "                ttfs_tensor[pol_ch, x, y, t_bin] += val\n",
    "                # Add temporal persistence\n",
    "                for future_bin in range(t_bin + 1, min(t_bin + 3, self.num_time_bins)):\n",
    "                    decay = 0.5 ** (future_bin - t_bin)\n",
    "                    ttfs_tensor[pol_ch, x, y, future_bin] += val * decay\n",
    "        \n",
    "        ttfs_tensor = np.clip(ttfs_tensor, 0, 10.0)\n",
    "        return torch.from_numpy(ttfs_tensor), label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NCarsDatasetForTTFS(ncars_data['train'], num_time_bins=10)\n",
    "test_dataset = NCarsDatasetForTTFS(ncars_data['test'], num_time_bins=10)\n",
    "\n",
    "print(f\"\\nTrain dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af7dd8e-aa4c-406a-b10c-af92d6e2b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: torch.Size([2, 120, 100, 10])\n",
      "Sample label: 1\n",
      "Data range: [0.00, 7.00]\n"
     ]
    }
   ],
   "source": [
    "# Check data format\n",
    "sample_data, sample_label = train_dataset[0]\n",
    "print(f\"Sample data shape: {sample_data.shape}\")  # Should be [2, 120, 100, 10]\n",
    "print(f\"Sample label: {sample_label}\")\n",
    "print(f\"Data range: [{sample_data.min():.2f}, {sample_data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa968dfa-3905-4f76-988c-e959c69e81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# # Split your dataset\n",
    "# train_size = int(0.95 * len(train_dataset))\n",
    "# val_size = len(train_dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72ea18c-5728-460c-87a6-95ae549f1243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† TTFS Spiking CNN with Majority Vote Classification (Enhanced)\n",
      "======================================================================\n",
      "üìÑ Implementation of 'High-performance deep spiking neural networks\n",
      "   with 0.3 spikes per neuron' (Nature Communications 2024)\n",
      "======================================================================\n",
      "üéØ Enhanced version with:\n",
      "   ‚Ä¢ Additional initial convolutional layer (conv0)\n",
      "   ‚Ä¢ Timestep-wise predictions with majority voting\n",
      "   ‚Ä¢ Multiple voting strategies (majority, average, weighted)\n",
      "   ‚Ä¢ Auxiliary timestep loss for better predictions\n",
      "   ‚Ä¢ Detailed voting and spike analysis\n",
      "   ‚Ä¢ Temporal pattern analysis\n",
      "   ‚Ä¢ Model comparison utilities\n",
      "   ‚Ä¢ Epoch-wise model saving\n",
      "======================================================================\n",
      "üì° Data format: [N, C=2, X=120, Y=100, T=10]\n",
      "   N: Batch size\n",
      "   C: 2 channels (e.g., polarity events)\n",
      "   X: 120 pixels width\n",
      "   Y: 100 pixels height\n",
      "   T: 10 temporal frames\n",
      "======================================================================\n",
      "üèóÔ∏è  Architecture: 4 conv layers + 2 FC layers\n",
      "   ‚Ä¢ Conv0: 2 ‚Üí 32 channels (NEW)\n",
      "   ‚Ä¢ Conv1: 32 ‚Üí 32 channels\n",
      "   ‚Ä¢ Conv2: 32 ‚Üí 64 channels\n",
      "   ‚Ä¢ Conv3: 64 ‚Üí 128 channels\n",
      "   ‚Ä¢ FC1: 128√ó7√ó6 ‚Üí 256\n",
      "   ‚Ä¢ FC2: 256 ‚Üí 64\n",
      "   ‚Ä¢ Output: 64 ‚Üí 2 classes\n",
      "======================================================================\n",
      "üìã Creating example data in format [N, C=2, X=120, Y=100, T=10]:\n",
      "   Data shape: torch.Size([8, 2, 120, 100, 10])\n",
      "   Labels shape: torch.Size([8])\n",
      "   Data type: torch.float32\n",
      "   Labels type: torch.int64\n",
      "\n",
      "üìù Usage Example:\n",
      "\n",
      "from torch.utils.data import DataLoader, TensorDataset\n",
      "\n",
      "# Create your dataset with shape [N, C=2, X=120, Y=100, T=10]\n",
      "dataset = TensorDataset(data_tensor, labels_tensor)\n",
      "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
      "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
      "\n",
      "# Train with different voting methods\n",
      "voting_methods = ['majority', 'average', 'weighted']\n",
      "\n",
      "for method in voting_methods:\n",
      "    print(f\"Training with {method} voting...\")\n",
      "\n",
      "    history = train_ncars_ttfs_snn_majority(\n",
      "        train_loader, \n",
      "        val_loader,\n",
      "        num_epochs=20,\n",
      "        voting_method=method,\n",
      "        timestep_loss_weight=0.1,  # Auxiliary loss weight\n",
      "        save_best=True,\n",
      "        save_all_epochs=True,\n",
      "        model_name_prefix=f'ttfs_snn_{method}'\n",
      "    )\n",
      "\n",
      "# List all saved models\n",
      "model_files = list_saved_models('models', 'ttfs_snn_majority')\n",
      "\n",
      "# Compare all saved models\n",
      "results = compare_models(model_files, test_loader, device)\n",
      "\n",
      "# Load and test specific model\n",
      "model, checkpoint = load_ttfs_model_majority('models/ttfs_snn_majority_best.pth', device)\n",
      "test_results = test_ncars_ttfs_snn_majority(model, test_loader, device)\n",
      "\n",
      "# Compare voting methods with same weights\n",
      "best_model_path = 'models/ttfs_snn_majority_best.pth'\n",
      "checkpoint = torch.load(best_model_path)\n",
      "comparison = compare_voting_methods(checkpoint['model_state_dict'], test_loader, device)\n",
      "\n",
      "# Analyze temporal patterns\n",
      "model, _ = load_ttfs_model_majority(best_model_path)\n",
      "temporal_analysis = analyze_temporal_patterns(model, test_loader, device)\n",
      "\n",
      "# Analyze spike patterns\n",
      "analyze_spike_patterns(history['spike_statistics'])\n",
      "\n",
      "# Benchmark efficiency\n",
      "benchmark_spike_efficiency(model, test_loader, device)\n",
      "\n",
      "# Save training history\n",
      "save_training_history(history, 'training_history.pth')\n",
      "\n",
      "\n",
      "üéØ Key Features:\n",
      "   ‚Ä¢ Additional initial conv layer for better feature extraction\n",
      "   ‚Ä¢ Timestep-wise predictions with final majority voting\n",
      "   ‚Ä¢ Multiple voting strategies: majority, average, weighted\n",
      "   ‚Ä¢ Auxiliary timestep loss for better individual predictions\n",
      "   ‚Ä¢ Detailed voting analysis and consensus tracking\n",
      "   ‚Ä¢ Temporal pattern analysis across timesteps\n",
      "   ‚Ä¢ Comparison utilities for different voting methods\n",
      "   ‚Ä¢ Early vs late prediction accuracy analysis\n",
      "   ‚Ä¢ Prediction stability and convergence metrics\n",
      "   ‚Ä¢ Epoch-wise model saving with complete metadata\n",
      "   ‚Ä¢ Best model tracking and automatic saving\n",
      "   ‚Ä¢ Spike efficiency benchmarking\n",
      "   ‚Ä¢ Enhanced depth with 4 convolutional layers\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from spikingjelly.clock_driven import functional\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "class TTFSNeuron(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-to-First-Spike (TTFS) neuron implementation based on IBM's identity mapping approach\n",
    "    Uses the B1-model (constant slope B=1) for equivalent training with ReLU networks\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, tau_c=1.0, v_threshold=1.0, v_reset=0.0, spike_count=True):\n",
    "        super().__init__()\n",
    "        self.shape = shape if isinstance(shape, (list, tuple)) else [shape]\n",
    "        self.tau_c = tau_c\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.spike_count = spike_count\n",
    "        \n",
    "        # B1-model parameters (identity mapping)\n",
    "        self.A = 0.0  # Initial slope = 0 \n",
    "        self.B = 1.0  # Fixed slope = 1 (identity mapping condition)\n",
    "        \n",
    "        # Time parameters (will be set dynamically)\n",
    "        self.t_min = 0.0\n",
    "        self.t_max = 10.0\n",
    "        \n",
    "        # For fully connected layers, create scalar threshold adjustment\n",
    "        if isinstance(shape, int):\n",
    "            self.threshold_adjustment = nn.Parameter(torch.zeros(shape))\n",
    "            self.num_neurons = shape\n",
    "        else:\n",
    "            # For conv layers, create per-channel threshold adjustment\n",
    "            self.threshold_adjustment = nn.Parameter(torch.zeros(shape[0]))\n",
    "            self.num_neurons = shape[0] if len(shape) > 0 else 1\n",
    "        \n",
    "        # Spike counting statistics\n",
    "        self.reset_spike_stats()\n",
    "        \n",
    "    def reset_spike_stats(self):\n",
    "        \"\"\"Reset spike counting statistics\"\"\"\n",
    "        self.total_spikes = 0\n",
    "        self.total_timesteps = 0\n",
    "        self.spikes_per_neuron = []\n",
    "        self.current_timestep_spikes = 0\n",
    "        \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset neuron state\"\"\"\n",
    "        pass  # TTFS neurons are stateless between time steps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for TTFS neuron - simplified to work like ReLU with learnable threshold\n",
    "        This maintains the mathematical equivalence while being trainable\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Apply learnable threshold adjustment\n",
    "        if len(x.shape) == 4:  # Conv layer [N, C, H, W]\n",
    "            # Reshape threshold for broadcasting: [C] -> [1, C, 1, 1]\n",
    "            threshold = self.threshold_adjustment.view(1, -1, 1, 1)\n",
    "            spatial_neurons = x.shape[2] * x.shape[3]  # H * W\n",
    "        elif len(x.shape) == 2:  # FC layer [N, features]\n",
    "            # Use threshold directly: [features]\n",
    "            threshold = self.threshold_adjustment\n",
    "            spatial_neurons = 1\n",
    "        else:\n",
    "            threshold = self.threshold_adjustment\n",
    "            spatial_neurons = 1\n",
    "        \n",
    "        # For identity mapping (B1-model), the equivalent ReLU operation is:\n",
    "        # output = max(0, input - threshold_adjustment)\n",
    "        # This is mathematically equivalent to the TTFS spike time conversion\n",
    "        output = torch.relu(x - threshold)\n",
    "        \n",
    "        # Calculate spikes for this timestep\n",
    "        spiking_mask = output > 0\n",
    "        \n",
    "        if self.spike_count:\n",
    "            # Count spikes per neuron in this timestep\n",
    "            if len(x.shape) == 4:  # Conv layer\n",
    "                # Sum spikes across spatial dimensions for each channel\n",
    "                spikes_per_channel = spiking_mask.sum(dim=(0, 2, 3)).float()\n",
    "                total_neurons_per_channel = batch_size * spatial_neurons\n",
    "                spikes_per_neuron = spikes_per_channel / total_neurons_per_channel\n",
    "            else:  # FC layer\n",
    "                # Sum spikes across batch dimension\n",
    "                spikes_per_feature = spiking_mask.sum(dim=0).float()\n",
    "                spikes_per_neuron = spikes_per_feature / batch_size\n",
    "            \n",
    "            self.spikes_per_neuron.append(spikes_per_neuron.detach().cpu())\n",
    "            self.current_timestep_spikes = spiking_mask.sum().item()\n",
    "            self.total_spikes += self.current_timestep_spikes\n",
    "            self.total_timesteps += 1\n",
    "        \n",
    "        # For compatibility, return format expected by calling code\n",
    "        spike_times = self.t_max - self.tau_c * output  # Reverse conversion for logging\n",
    "        \n",
    "        return output, spike_times, spiking_mask\n",
    "    \n",
    "    def get_spike_statistics(self):\n",
    "        \"\"\"Get spike statistics for this neuron layer\"\"\"\n",
    "        if not self.spikes_per_neuron:\n",
    "            return {\n",
    "                'avg_spikes_per_neuron': 0.0,\n",
    "                'total_spikes': 0,\n",
    "                'total_timesteps': 0,\n",
    "                'spike_rate': 0.0\n",
    "            }\n",
    "        \n",
    "        # Average spikes per neuron across all timesteps\n",
    "        all_spikes = torch.stack(self.spikes_per_neuron)  # [timesteps, neurons]\n",
    "        avg_spikes_per_neuron = all_spikes.mean().item()\n",
    "        \n",
    "        return {\n",
    "            'avg_spikes_per_neuron': avg_spikes_per_neuron,\n",
    "            'total_spikes': self.total_spikes,\n",
    "            'total_timesteps': self.total_timesteps,\n",
    "            'spike_rate': self.total_spikes / max(self.total_timesteps, 1),\n",
    "            'spikes_per_timestep': all_spikes.mean(dim=1).tolist()  # Average spikes per timestep\n",
    "        }\n",
    "\n",
    "class TTFSConvNeuron(TTFSNeuron):\n",
    "    \"\"\"TTFS neuron for convolutional layers\"\"\"\n",
    "    def __init__(self, channels, height=None, width=None, **kwargs):\n",
    "        # For conv layers, we only need the number of channels for threshold adjustment\n",
    "        # The spatial dimensions (height, width) are handled automatically by broadcasting\n",
    "        super().__init__([channels], **kwargs)\n",
    "\n",
    "class SpikingCNN_NCARS_TTFS(nn.Module):\n",
    "    \"\"\"\n",
    "    TTFS-based Spiking CNN implementing IBM's equivalent training method with majority voting\n",
    "    Uses identity mapping (B1-model) for exact equivalence with ReLU networks\n",
    "    \n",
    "    Modified for data shape: [N, C=2, X=120, Y=100, T=10]\n",
    "    Now predicts for each timestep and uses majority voting for final classification\n",
    "    Enhanced with an additional initial convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, tau_c=1.0, v_threshold=1.0, v_reset=0.0, num_classes=2, \n",
    "                 voting_method='majority', confidence_weighted=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tau_c = tau_c\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.num_classes = num_classes\n",
    "        self.voting_method = voting_method  # 'majority', 'average', 'weighted'\n",
    "        self.confidence_weighted = confidence_weighted\n",
    "        \n",
    "        # Time parameters for each layer (will be adaptive) - added one more for new layer\n",
    "        self.t_max = [0.0, 5.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0]\n",
    "        self.t_min = [0.0, 5.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0]\n",
    "        \n",
    "        # New initial convolutional block - input has 2 channels\n",
    "        self.conv0 = nn.Conv2d(2, 32, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.bn0 = nn.BatchNorm2d(32)\n",
    "        self.ttfs0 = TTFSConvNeuron(32, 120, 100, tau_c=tau_c, v_threshold=v_threshold)\n",
    "        self.pool0 = nn.AvgPool2d(kernel_size=2, stride=2)  # Average pooling for TTFS\n",
    "        \n",
    "        # First convolutional block - now takes 32 channels from conv0\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.ttfs1 = TTFSConvNeuron(32, 60, 50, tau_c=tau_c, v_threshold=v_threshold)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # Average pooling for TTFS\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.ttfs2 = TTFSConvNeuron(64, 30, 25, tau_c=tau_c, v_threshold=v_threshold)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.ttfs3 = TTFSConvNeuron(128, 15, 12, tau_c=tau_c, v_threshold=v_threshold)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers - adjusted for new spatial dimensions after 4 pooling layers\n",
    "        # After 4 pooling layers (2x each): 120 -> 60 -> 30 -> 15 -> 7 (with rounding)\n",
    "        # And: 100 -> 50 -> 25 -> 12 -> 6 (with rounding)\n",
    "        self.fc1 = nn.Linear(128 * 7 * 6, 256, bias=False)\n",
    "        self.ttfs4 = TTFSNeuron(256, tau_c=tau_c, v_threshold=v_threshold)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 64, bias=False)\n",
    "        self.ttfs5 = TTFSNeuron(64, tau_c=tau_c, v_threshold=v_threshold)\n",
    "        \n",
    "        # Output layer (non-spiking readout)\n",
    "        self.fc3 = nn.Linear(64, num_classes, bias=False)\n",
    "        \n",
    "        # Optional confidence network for weighted voting\n",
    "        if confidence_weighted:\n",
    "            self.confidence_net = nn.Sequential(\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        # Initialize weights using method compatible with identity mapping\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Layer names for spike statistics - added ttfs0\n",
    "        self.spike_layers = ['ttfs0', 'ttfs1', 'ttfs2', 'ttfs3', 'ttfs4', 'ttfs5']\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for stable TTFS training\n",
    "        Based on IBM's identity mapping approach\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                # Use He initialization for ReLU-like behavior\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def reset_spike_statistics(self):\n",
    "        \"\"\"Reset spike statistics for all TTFS layers\"\"\"\n",
    "        for layer_name in self.spike_layers:\n",
    "            layer = getattr(self, layer_name)\n",
    "            layer.reset_spike_stats()\n",
    "    \n",
    "    def get_network_spike_statistics(self):\n",
    "        \"\"\"Get spike statistics for the entire network\"\"\"\n",
    "        stats = {}\n",
    "        total_network_spikes = 0\n",
    "        total_network_neurons = 0\n",
    "        \n",
    "        for layer_name in self.spike_layers:\n",
    "            layer = getattr(self, layer_name)\n",
    "            layer_stats = layer.get_spike_statistics()\n",
    "            stats[layer_name] = layer_stats\n",
    "            \n",
    "            # Accumulate network totals\n",
    "            total_network_spikes += layer_stats['total_spikes']\n",
    "            total_network_neurons += layer.num_neurons\n",
    "        \n",
    "        # Calculate network-wide averages\n",
    "        stats['network_summary'] = {\n",
    "            'total_spikes': total_network_spikes,\n",
    "            'total_neurons': total_network_neurons,\n",
    "            'avg_spikes_per_neuron': total_network_spikes / max(total_network_neurons, 1),\n",
    "            'layers': len(self.spike_layers)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def update_time_windows(self, layer_outputs):\n",
    "        \"\"\"\n",
    "        Adaptive time window update to ensure spikes occur within bounds\n",
    "        \"\"\"\n",
    "        gamma = 0.1  # Safety margin\n",
    "        \n",
    "        for i, output in enumerate(layer_outputs):\n",
    "            if output is not None and len(output) > 1:  # Has spike times\n",
    "                spike_times = output[1]  # Get spike times\n",
    "                if spike_times.numel() > 0:\n",
    "                    # Filter spike times within current window\n",
    "                    valid_spikes = spike_times[spike_times < self.t_max[i]]\n",
    "                    if valid_spikes.numel() > 0:  # Check if any valid spikes exist\n",
    "                        max_spike_time = torch.max(valid_spikes)\n",
    "                        if max_spike_time.item() > 0:\n",
    "                            # Update t_max with safety margin\n",
    "                            new_t_max = max_spike_time.item() + gamma * (self.t_max[i] - self.t_min[i])\n",
    "                            self.t_max[i] = min(new_t_max, self.t_max[i])\n",
    "                            \n",
    "                            # Update subsequent time windows\n",
    "                            if i + 1 < len(self.t_min):\n",
    "                                self.t_min[i + 1] = self.t_max[i]\n",
    "    \n",
    "    def reset_neurons(self):\n",
    "        \"\"\"Reset all TTFS neurons\"\"\"\n",
    "        self.ttfs0.reset_state()\n",
    "        self.ttfs1.reset_state()\n",
    "        self.ttfs2.reset_state()\n",
    "        self.ttfs3.reset_state()\n",
    "        self.ttfs4.reset_state()\n",
    "        self.ttfs5.reset_state()\n",
    "    \n",
    "    def apply_majority_vote(self, timestep_outputs, timestep_confidences=None):\n",
    "        \"\"\"\n",
    "        Apply majority voting across timestep predictions\n",
    "        \n",
    "        Args:\n",
    "            timestep_outputs: List of [batch_size, num_classes] tensors, one per timestep\n",
    "            timestep_confidences: Optional list of confidence scores per timestep\n",
    "            \n",
    "        Returns:\n",
    "            final_output: [batch_size, num_classes] final prediction\n",
    "            voting_info: Dictionary with voting statistics\n",
    "        \"\"\"\n",
    "        if not timestep_outputs:\n",
    "            raise ValueError(\"No timestep outputs provided for voting\")\n",
    "        \n",
    "        batch_size = timestep_outputs[0].shape[0]\n",
    "        num_timesteps = len(timestep_outputs)\n",
    "        \n",
    "        if self.voting_method == 'majority':\n",
    "            # Hard majority voting\n",
    "            timestep_predictions = torch.stack([torch.argmax(output, dim=1) for output in timestep_outputs], dim=1)\n",
    "            \n",
    "            # Count votes for each class\n",
    "            vote_counts = torch.zeros(batch_size, self.num_classes, device=timestep_predictions.device)\n",
    "            for i in range(batch_size):\n",
    "                for t in range(num_timesteps):\n",
    "                    pred_class = timestep_predictions[i, t]\n",
    "                    vote_counts[i, pred_class] += 1\n",
    "            \n",
    "            # Final prediction is the class with most votes\n",
    "            final_predictions = torch.argmax(vote_counts, dim=1)\n",
    "            \n",
    "            # Create one-hot encoded output for compatibility with loss functions\n",
    "            final_output = torch.zeros(batch_size, self.num_classes, device=timestep_predictions.device)\n",
    "            final_output.scatter_(1, final_predictions.unsqueeze(1), 1.0)\n",
    "            \n",
    "            # Convert to logit-like scores (add small epsilon to avoid log(0))\n",
    "            final_output = torch.log(final_output + 1e-8)\n",
    "            \n",
    "            voting_info = {\n",
    "                'timestep_predictions': timestep_predictions,\n",
    "                'vote_counts': vote_counts,\n",
    "                'final_predictions': final_predictions,\n",
    "                'consensus_strength': vote_counts.max(dim=1)[0] / num_timesteps  # Fraction of votes for winner\n",
    "            }\n",
    "            \n",
    "        elif self.voting_method == 'average':\n",
    "            # Soft voting - average the logits/outputs\n",
    "            stacked_outputs = torch.stack(timestep_outputs, dim=1)  # [batch, timesteps, classes]\n",
    "            \n",
    "            if timestep_confidences is not None:\n",
    "                # Confidence-weighted average\n",
    "                confidences = torch.stack(timestep_confidences, dim=1)  # [batch, timesteps, 1]\n",
    "                weights = F.softmax(confidences, dim=1)\n",
    "                final_output = (stacked_outputs * weights).sum(dim=1)\n",
    "            else:\n",
    "                # Simple average\n",
    "                final_output = stacked_outputs.mean(dim=1)\n",
    "            \n",
    "            final_predictions = torch.argmax(final_output, dim=1)\n",
    "            \n",
    "            voting_info = {\n",
    "                'timestep_outputs': stacked_outputs,\n",
    "                'final_predictions': final_predictions,\n",
    "                'weights': weights if timestep_confidences is not None else None,\n",
    "                'consensus_strength': F.softmax(final_output, dim=1).max(dim=1)[0]  # Max probability\n",
    "            }\n",
    "            \n",
    "        elif self.voting_method == 'weighted':\n",
    "            # Weighted voting based on prediction confidence\n",
    "            stacked_outputs = torch.stack(timestep_outputs, dim=1)  # [batch, timesteps, classes]\n",
    "            \n",
    "            # Use max probability as confidence measure\n",
    "            timestep_probs = F.softmax(stacked_outputs, dim=2)\n",
    "            timestep_confidences = timestep_probs.max(dim=2)[0]  # [batch, timesteps]\n",
    "            \n",
    "            # Normalize confidences to create weights\n",
    "            weights = F.softmax(timestep_confidences, dim=1).unsqueeze(2)  # [batch, timesteps, 1]\n",
    "            \n",
    "            # Weighted average\n",
    "            final_output = (stacked_outputs * weights).sum(dim=1)\n",
    "            final_predictions = torch.argmax(final_output, dim=1)\n",
    "            \n",
    "            voting_info = {\n",
    "                'timestep_outputs': stacked_outputs,\n",
    "                'timestep_confidences': timestep_confidences,\n",
    "                'weights': weights.squeeze(2),\n",
    "                'final_predictions': final_predictions,\n",
    "                'consensus_strength': F.softmax(final_output, dim=1).max(dim=1)[0]\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown voting method: {self.voting_method}\")\n",
    "        \n",
    "        return final_output, voting_info\n",
    "    \n",
    "    def forward(self, x, return_timestep_outputs=False):\n",
    "        \"\"\"\n",
    "        Forward pass processing each timestep sequentially\n",
    "        Input shape: [N, C=2, X=120, Y=100, T=10]\n",
    "        \"\"\"\n",
    "        # Data should already be in shape [N, C=2, X=120, Y=100, T=10]\n",
    "        # If your data comes in a different format, adjust here:\n",
    "        # Example: if data is [N, T, C, X, Y], permute to [N, C, X, Y, T]\n",
    "        # x = x.permute(0, 2, 3, 4, 1)\n",
    "        \n",
    "        N, C, X, Y, T = x.shape\n",
    "        \n",
    "        # Verify expected input format\n",
    "        assert C == 2, f\"Expected 2 channels, got {C}\"\n",
    "        assert X == 120, f\"Expected width 120, got {X}\"\n",
    "        assert Y == 100, f\"Expected height 100, got {Y}\"\n",
    "        assert T == 10, f\"Expected 10 timesteps, got {T}\"\n",
    "        \n",
    "        # Process each time step and collect outputs\n",
    "        timestep_outputs = []\n",
    "        timestep_confidences = [] if self.confidence_weighted else None\n",
    "        \n",
    "        # Reset spike statistics at the beginning of forward pass\n",
    "        if self.training:\n",
    "            self.reset_spike_statistics()\n",
    "        \n",
    "        for t in range(T):\n",
    "            # Extract frame at time t: [N, C, X, Y]\n",
    "            x_t = x[:, :, :, :, t]\n",
    "            \n",
    "            # New initial conv block\n",
    "            x_t = self.conv0(x_t)\n",
    "            x_t = self.bn0(x_t)\n",
    "            x_t, spike_times0, mask0 = self.ttfs0(x_t)\n",
    "            x_t = self.pool0(x_t)\n",
    "            \n",
    "            # First conv block\n",
    "            x_t = self.conv1(x_t)\n",
    "            x_t = self.bn1(x_t)\n",
    "            x_t, spike_times1, mask1 = self.ttfs1(x_t)\n",
    "            x_t = self.pool1(x_t)\n",
    "            \n",
    "            # Second conv block  \n",
    "            x_t = self.conv2(x_t)\n",
    "            x_t = self.bn2(x_t)\n",
    "            x_t, spike_times2, mask2 = self.ttfs2(x_t)\n",
    "            x_t = self.pool2(x_t)\n",
    "            \n",
    "            # Third conv block\n",
    "            x_t = self.conv3(x_t)\n",
    "            x_t = self.bn3(x_t)\n",
    "            x_t, spike_times3, mask3 = self.ttfs3(x_t)\n",
    "            x_t = self.pool3(x_t)\n",
    "            \n",
    "            # Flatten\n",
    "            x_t = x_t.reshape(N, -1)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            x_t = self.fc1(x_t)\n",
    "            x_t, spike_times4, mask4 = self.ttfs4(x_t)\n",
    "            \n",
    "            x_t = self.fc2(x_t)\n",
    "            x_t, spike_times5, mask5 = self.ttfs5(x_t)\n",
    "            \n",
    "            # Output layer (non-spiking)\n",
    "            output_t = self.fc3(x_t)\n",
    "            timestep_outputs.append(output_t)\n",
    "            \n",
    "            # Optional confidence prediction\n",
    "            if self.confidence_weighted:\n",
    "                confidence_t = self.confidence_net(x_t)\n",
    "                timestep_confidences.append(confidence_t)\n",
    "        \n",
    "        # Apply majority voting across timesteps\n",
    "        final_output, voting_info = self.apply_majority_vote(timestep_outputs, timestep_confidences)\n",
    "        \n",
    "        if return_timestep_outputs:\n",
    "            return final_output, timestep_outputs, voting_info\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "def train_ncars_ttfs_snn_majority(train_loader, val_loader=None, num_epochs=20, log_spikes=True, \n",
    "                                 save_best=True, save_all_epochs=True, \n",
    "                                 model_save_dir='models', model_name_prefix='ttfs_snn_majority',\n",
    "                                 voting_method='majority', confidence_weighted=False,\n",
    "                                 timestep_loss_weight=0.1):\n",
    "    \"\"\"\n",
    "    Train TTFS SNN with majority voting using both final and timestep losses\n",
    "    Modified to match Code 2's data handling and display format\n",
    "    \n",
    "    Args:\n",
    "        timestep_loss_weight: Weight for auxiliary timestep losses (0.0 to disable)\n",
    "        voting_method: 'majority', 'average', or 'weighted'\n",
    "        confidence_weighted: Whether to use confidence weighting\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    if save_all_epochs or save_best:\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        print(f\"üìÅ Model save directory: {model_save_dir}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üöÄ Starting TTFS SNN training with {voting_method} voting on device: {device}\")\n",
    "    print(f\"üìä Training samples: {len(train_loader.dataset)}\")\n",
    "    if val_loader:\n",
    "        print(f\"üìä Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"üéØ Number of epochs: {num_epochs}\")\n",
    "    print(f\"üì° Data format: [N, C=2, X=120, Y=100, T=10]\")\n",
    "    print(f\"üó≥Ô∏è  Voting method: {voting_method}\")\n",
    "    print(f\"‚öñÔ∏è  Timestep loss weight: {timestep_loss_weight}\")\n",
    "    print(f\"üß† Architecture: 4 conv layers + 2 FC layers (with new initial conv layer)\")\n",
    "    if save_best:\n",
    "        print(f\"üíæ Best model will be saved to: {model_save_dir}\")\n",
    "    if save_all_epochs:\n",
    "        print(f\"üíæ All epoch models will be saved to: {model_save_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Model, loss, optimizer\n",
    "    model = SpikingCNN_NCARS_TTFS(tau_c=1.0, v_threshold=1.0, v_reset=0.0, num_classes=2,\n",
    "                                 voting_method=voting_method, confidence_weighted=confidence_weighted).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"üß† TTFS SNN parameters: {total_params:,}\")\n",
    "    print(f\"üéØ Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"‚ö° Using Time-to-First-Spike coding with {voting_method} voting\")\n",
    "    print(f\"üî¨ Method: IBM equivalent training (Nature Communications 2024)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    spike_statistics_history = []\n",
    "    voting_statistics_history = []\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        total_final_loss = 0\n",
    "        total_timestep_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', \n",
    "                   unit='batch', leave=False)\n",
    "        \n",
    "        # Reset statistics\n",
    "        epoch_spike_stats = []\n",
    "        epoch_voting_stats = []\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Data should already be in shape [N, C=2, X=120, Y=100, T=10]\n",
    "            # If your data comes in a different format, adjust here:\n",
    "            # Example: if data is [N, T, C, X, Y], permute to [N, C, X, Y, T]\n",
    "            # data = data.permute(0, 2, 3, 4, 1)\n",
    "            \n",
    "            # Forward pass with timestep outputs\n",
    "            final_output, timestep_outputs, voting_info = model(data, return_timestep_outputs=True)\n",
    "            \n",
    "            # Main loss on final majority vote output\n",
    "            final_loss = criterion(final_output, target)\n",
    "            \n",
    "            # Optional auxiliary loss on individual timestep predictions\n",
    "            timestep_loss = 0.0\n",
    "            if timestep_loss_weight > 0.0:\n",
    "                for t_output in timestep_outputs:\n",
    "                    timestep_loss += criterion(t_output, target)\n",
    "                timestep_loss = timestep_loss / len(timestep_outputs)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = final_loss + timestep_loss_weight * timestep_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            total_final_loss += final_loss.item()\n",
    "            total_timestep_loss += timestep_loss if isinstance(timestep_loss, (int, float)) else timestep_loss.item()\n",
    "            \n",
    "            # Accuracy calculation\n",
    "            if voting_method == 'majority':\n",
    "                predicted = voting_info['final_predictions']\n",
    "            else:\n",
    "                _, predicted = final_output.max(1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            # Log statistics\n",
    "            if log_spikes and batch_idx % 10 == 0:\n",
    "                batch_spike_stats = model.get_network_spike_statistics()\n",
    "                epoch_spike_stats.append(batch_spike_stats)\n",
    "                \n",
    "                # Voting statistics\n",
    "                consensus = voting_info['consensus_strength'].mean().item()\n",
    "                epoch_voting_stats.append({\n",
    "                    'consensus_strength': consensus,\n",
    "                    'voting_method': voting_method\n",
    "                })\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_acc = 100. * correct / total\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'FinalL': f'{final_loss.item():.4f}',\n",
    "                'TsL': f'{timestep_loss:.4f}' if isinstance(timestep_loss, (int, float)) else f'{timestep_loss.item():.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Step learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_final_loss = total_final_loss / len(train_loader)\n",
    "        avg_timestep_loss = total_timestep_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        # Statistics logging\n",
    "        if epoch_spike_stats:\n",
    "            avg_spikes_per_neuron = np.mean([stats['network_summary']['avg_spikes_per_neuron'] \n",
    "                                           for stats in epoch_spike_stats])\n",
    "            avg_consensus = np.mean([stats['consensus_strength'] for stats in epoch_voting_stats])\n",
    "            \n",
    "            spike_statistics_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_spikes_per_neuron': avg_spikes_per_neuron,\n",
    "                'detailed_stats': epoch_spike_stats[-1]\n",
    "            })\n",
    "            \n",
    "            voting_statistics_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_consensus_strength': avg_consensus,\n",
    "                'voting_method': voting_method\n",
    "            })\n",
    "        \n",
    "        print(f\"üìà Epoch {epoch+1:2d} | \"\n",
    "              f\"Loss: {avg_loss:.4f} (Final: {avg_final_loss:.4f}, TS: {avg_timestep_loss:.4f}) | \"\n",
    "              f\"Train Acc: {train_acc:6.2f}% | \"\n",
    "              f\"Time: {epoch_time:5.1f}s | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        # Print additional statistics\n",
    "        if log_spikes and epoch_spike_stats:\n",
    "            avg_spikes = np.mean([stats['network_summary']['avg_spikes_per_neuron'] \n",
    "                                for stats in epoch_spike_stats])\n",
    "            avg_consensus = np.mean([stats['consensus_strength'] for stats in epoch_voting_stats])\n",
    "            print(f\"‚ö° Avg spikes/neuron: {avg_spikes:.3f} | Voting consensus: {avg_consensus:.3f}\")\n",
    "            \n",
    "            # Print detailed layer statistics\n",
    "            last_stats = epoch_spike_stats[-1]\n",
    "            for layer_name in ['ttfs0', 'ttfs1', 'ttfs2', 'ttfs3', 'ttfs4', 'ttfs5']:\n",
    "                layer_spikes = last_stats[layer_name]['avg_spikes_per_neuron']\n",
    "                print(f\"   {layer_name}: {layer_spikes:.3f} spikes/neuron\")\n",
    "        \n",
    "        # Validation\n",
    "        current_accuracy = train_acc  # Default to training accuracy\n",
    "        if val_loader is not None:\n",
    "            print(\"üîç Running validation...\", end=\" \")\n",
    "            val_acc = test_ncars_ttfs_snn_majority(model, val_loader, device, verbose=False)\n",
    "            val_accuracies.append(val_acc)\n",
    "            current_accuracy = val_acc\n",
    "            print(f\"Val Acc: {val_acc:6.2f}%\")\n",
    "            model.train()\n",
    "        \n",
    "        # Create epoch model metadata\n",
    "        epoch_metadata = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_accuracy': val_acc if val_loader else None,\n",
    "            'train_loss': avg_loss,\n",
    "            'final_loss': avg_final_loss,\n",
    "            'timestep_loss': avg_timestep_loss,\n",
    "            'voting_method': voting_method,\n",
    "            'confidence_weighted': confidence_weighted,\n",
    "            'timestep_loss_weight': timestep_loss_weight,\n",
    "            'learning_rate': scheduler.get_last_lr()[0],\n",
    "            'spike_statistics': epoch_spike_stats[-1] if epoch_spike_stats else None,\n",
    "            'voting_statistics': epoch_voting_stats[-1] if epoch_voting_stats else None,\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'total_epochs': num_epochs,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        \n",
    "        # Save models\n",
    "        if save_all_epochs:\n",
    "            epoch_model_path = os.path.join(model_save_dir, f\"{model_name_prefix}_epoch_{epoch+1:02d}.pth\")\n",
    "            save_ttfs_model_state(model.state_dict(), epoch_model_path, epoch_metadata)\n",
    "            print(f\"üíæ Epoch {epoch+1} model saved: {epoch_model_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if save_best and current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            best_metadata = epoch_metadata.copy()\n",
    "            best_metadata.update({\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_epoch': best_epoch,\n",
    "                'model_type': 'best_model',\n",
    "            })\n",
    "            \n",
    "            best_model_path = os.path.join(model_save_dir, f\"{model_name_prefix}_best.pth\")\n",
    "            save_ttfs_model_state(best_model_state, best_model_path, best_metadata)\n",
    "            print(f\"üèÜ New best model saved! Accuracy: {best_accuracy:.2f}% (Epoch {best_epoch})\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Save final model\n",
    "    if save_all_epochs:\n",
    "        final_model_path = os.path.join(model_save_dir, f\"{model_name_prefix}_final.pth\")\n",
    "        final_metadata = epoch_metadata.copy()\n",
    "        final_metadata.update({\n",
    "            'model_type': 'final_model',\n",
    "            'final_train_accuracy': train_acc,\n",
    "            'final_val_accuracy': val_acc if val_loader else None,\n",
    "        })\n",
    "        save_ttfs_model_state(model.state_dict(), final_model_path, final_metadata)\n",
    "        print(f\"üîö Final model saved: {final_model_path}\")\n",
    "    \n",
    "    # Training summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"üéâ TTFS SNN majority voting training completed!\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"üìà Best training accuracy: {max(train_accuracies):.2f}%\")\n",
    "    if val_accuracies:\n",
    "        print(f\"üìä Best validation accuracy: {max(val_accuracies):.2f}%\")\n",
    "    \n",
    "    # Print saved model information\n",
    "    print(f\"\\nüíæ Saved Models Summary:\")\n",
    "    if save_all_epochs:\n",
    "        print(f\"   üìÅ Epoch models: {model_save_dir}/{model_name_prefix}_epoch_XX.pth\")\n",
    "        print(f\"   üìÅ Final model: {model_save_dir}/{model_name_prefix}_final.pth\")\n",
    "    if save_best:\n",
    "        print(f\"   üèÜ Best model: {model_save_dir}/{model_name_prefix}_best.pth\")\n",
    "        print(f\"       Best accuracy: {best_accuracy:.2f}% at epoch {best_epoch}\")\n",
    "    \n",
    "    # Final spike statistics summary\n",
    "    if spike_statistics_history:\n",
    "        final_spikes = spike_statistics_history[-1]['avg_spikes_per_neuron']\n",
    "        print(f\"‚ö° Final average spikes per neuron: {final_spikes:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies if val_accuracies else None,\n",
    "        'spike_statistics': spike_statistics_history,\n",
    "        'voting_statistics': voting_statistics_history,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'voting_method': voting_method,\n",
    "        'model_save_dir': model_save_dir,\n",
    "        'saved_models': {\n",
    "            'best_model': f\"{model_save_dir}/{model_name_prefix}_best.pth\" if save_best else None,\n",
    "            'final_model': f\"{model_save_dir}/{model_name_prefix}_final.pth\" if save_all_epochs else None,\n",
    "            'epoch_models': [f\"{model_save_dir}/{model_name_prefix}_epoch_{i+1:02d}.pth\" \n",
    "                           for i in range(num_epochs)] if save_all_epochs else None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def test_ncars_ttfs_snn_majority(model, test_loader, device, verbose=True, log_spikes=True, analyze_voting=True):\n",
    "    \"\"\"Test the trained TTFS SNN with majority voting and detailed analysis\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üß™ Testing TTFS SNN model with majority voting...\")\n",
    "        pbar = tqdm(test_loader, desc='Testing', unit='batch')\n",
    "    else:\n",
    "        pbar = test_loader\n",
    "    \n",
    "    test_spike_stats = []\n",
    "    voting_analysis = {\n",
    "        'consensus_scores': [],\n",
    "        'timestep_accuracies': [[] for _ in range(10)],  # Assuming T=10\n",
    "        'final_accuracies': [],\n",
    "        'disagreement_cases': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass with timestep outputs\n",
    "            final_output, timestep_outputs, voting_info = model(data, return_timestep_outputs=True)\n",
    "            \n",
    "            # Compute loss\n",
    "            test_loss += criterion(final_output, target).item()\n",
    "            \n",
    "            # Accuracy calculation\n",
    "            if model.voting_method == 'majority':\n",
    "                predicted = voting_info['final_predictions']\n",
    "            else:\n",
    "                _, predicted = final_output.max(1)\n",
    "            \n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            # Detailed voting analysis\n",
    "            if analyze_voting:\n",
    "                consensus = voting_info['consensus_strength']\n",
    "                voting_analysis['consensus_scores'].extend(consensus.cpu().tolist())\n",
    "                \n",
    "                # Analyze individual timestep accuracies\n",
    "                for t, t_output in enumerate(timestep_outputs):\n",
    "                    _, t_pred = t_output.max(1)\n",
    "                    t_acc = t_pred.eq(target).float()\n",
    "                    voting_analysis['timestep_accuracies'][t].extend(t_acc.cpu().tolist())\n",
    "                \n",
    "                # Final accuracy for this batch\n",
    "                final_acc = predicted.eq(target).float()\n",
    "                voting_analysis['final_accuracies'].extend(final_acc.cpu().tolist())\n",
    "                \n",
    "                # Find disagreement cases (where timesteps disagree significantly)\n",
    "                if model.voting_method == 'majority' and 'timestep_predictions' in voting_info:\n",
    "                    timestep_preds = voting_info['timestep_predictions']\n",
    "                    for i in range(timestep_preds.shape[0]):\n",
    "                        sample_preds = timestep_preds[i]  # [T]\n",
    "                        unique_preds = torch.unique(sample_preds)\n",
    "                        if len(unique_preds) > 1:  # Disagreement exists\n",
    "                            disagreement_ratio = 1.0 - (sample_preds == sample_preds[0]).float().mean().item()\n",
    "                            if disagreement_ratio > 0.3:  # Significant disagreement\n",
    "                                voting_analysis['disagreement_cases'].append({\n",
    "                                    'sample_idx': batch_idx * data.shape[0] + i,\n",
    "                                    'timestep_predictions': sample_preds.cpu().tolist(),\n",
    "                                    'final_prediction': predicted[i].item(),\n",
    "                                    'true_label': target[i].item(),\n",
    "                                    'disagreement_ratio': disagreement_ratio,\n",
    "                                    'consensus_strength': consensus[i].item()\n",
    "                                })\n",
    "            \n",
    "            # Log spike statistics\n",
    "            if log_spikes and batch_idx % 10 == 0:\n",
    "                batch_spike_stats = model.get_network_spike_statistics()\n",
    "                test_spike_stats.append(batch_spike_stats)\n",
    "            \n",
    "            if verbose and hasattr(pbar, 'set_postfix'):\n",
    "                current_acc = 100. * correct / ((batch_idx + 1) * data.shape[0])\n",
    "                pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})\n",
    "    \n",
    "    if verbose and hasattr(pbar, 'close'):\n",
    "        pbar.close()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ TTFS SNN Test Results with {model.voting_method} voting:\")\n",
    "        print(f\"   Loss: {test_loss:.4f}\")\n",
    "        print(f\"   Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Voting analysis results\n",
    "        if analyze_voting:\n",
    "            avg_consensus = np.mean(voting_analysis['consensus_scores'])\n",
    "            print(f\"   Average consensus strength: {avg_consensus:.3f}\")\n",
    "            \n",
    "            # Timestep accuracy analysis\n",
    "            print(\"   Per-timestep accuracies:\")\n",
    "            for t in range(len(voting_analysis['timestep_accuracies'])):\n",
    "                if voting_analysis['timestep_accuracies'][t]:\n",
    "                    t_acc = np.mean(voting_analysis['timestep_accuracies'][t]) * 100\n",
    "                    print(f\"     Timestep {t+1}: {t_acc:.2f}%\")\n",
    "            \n",
    "            # Disagreement analysis\n",
    "            if voting_analysis['disagreement_cases']:\n",
    "                print(f\"   High disagreement cases: {len(voting_analysis['disagreement_cases'])}\")\n",
    "                print(f\"   Average disagreement ratio: {np.mean([case['disagreement_ratio'] for case in voting_analysis['disagreement_cases']]):.3f}\")\n",
    "        \n",
    "        # Spike statistics\n",
    "        if test_spike_stats:\n",
    "            avg_spikes = np.mean([stats['network_summary']['avg_spikes_per_neuron'] \n",
    "                                for stats in test_spike_stats])\n",
    "            print(f\"   Average spikes per neuron: {avg_spikes:.3f}\")\n",
    "            \n",
    "            # Detailed layer statistics\n",
    "            last_stats = test_spike_stats[-1]\n",
    "            print(\"üìä Per-layer spike statistics:\")\n",
    "            for layer_name in ['ttfs0', 'ttfs1', 'ttfs2', 'ttfs3', 'ttfs4', 'ttfs5']:\n",
    "                layer_spikes = last_stats[layer_name]['avg_spikes_per_neuron']\n",
    "                print(f\"   {layer_name}: {layer_spikes:.3f} spikes/neuron\")\n",
    "    \n",
    "    test_results = {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': test_loss,\n",
    "        'voting_analysis': voting_analysis if analyze_voting else None,\n",
    "        'spike_statistics': test_spike_stats\n",
    "    }\n",
    "    \n",
    "    return accuracy if not verbose else test_results\n",
    "\n",
    "def compare_voting_methods(model_state_dict, test_loader, device='cpu', num_classes=2):\n",
    "    \"\"\"Compare different voting methods using the same trained model weights\"\"\"\n",
    "    print(\"üîç Comparing voting methods with same trained weights:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    voting_methods = ['majority', 'average', 'weighted']\n",
    "    results = {}\n",
    "    \n",
    "    for method in voting_methods:\n",
    "        print(f\"\\nüó≥Ô∏è  Testing with {method} voting:\")\n",
    "        \n",
    "        # Create model with specific voting method\n",
    "        model = SpikingCNN_NCARS_TTFS(num_classes=num_classes, voting_method=method).to(device)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # Test the model\n",
    "        test_results = test_ncars_ttfs_snn_majority(model, test_loader, device, \n",
    "                                                   verbose=True, log_spikes=False, analyze_voting=True)\n",
    "        \n",
    "        results[method] = test_results\n",
    "        print(f\"   {method.title()} voting accuracy: {test_results['accuracy']:.2f}%\")\n",
    "        if test_results['voting_analysis']:\n",
    "            avg_consensus = np.mean(test_results['voting_analysis']['consensus_scores'])\n",
    "            print(f\"   Average consensus: {avg_consensus:.3f}\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\nüìä Voting Method Comparison Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    for method in voting_methods:\n",
    "        acc = results[method]['accuracy']\n",
    "        consensus = np.mean(results[method]['voting_analysis']['consensus_scores']) if results[method]['voting_analysis'] else 0\n",
    "        print(f\"{method.title():<10}: {acc:6.2f}% (consensus: {consensus:.3f})\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "    print(f\"\\nüèÜ Best voting method: {best_method.title()} ({results[best_method]['accuracy']:.2f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_temporal_patterns(model, test_loader, device, num_samples=100):\n",
    "    \"\"\"Analyze how predictions change across timesteps\"\"\"\n",
    "    print(\"\\nüìä Analyzing temporal prediction patterns...\")\n",
    "    \n",
    "    model.eval()\n",
    "    temporal_analysis = {\n",
    "        'timestep_confidences': [[] for _ in range(10)],\n",
    "        'prediction_stability': [],\n",
    "        'early_vs_late_accuracy': {'early': [], 'late': []},\n",
    "        'convergence_patterns': []\n",
    "    }\n",
    "    \n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get timestep outputs\n",
    "            final_output, timestep_outputs, voting_info = model(data, return_timestep_outputs=True)\n",
    "            \n",
    "            batch_size = data.shape[0]\n",
    "            for i in range(min(batch_size, num_samples - sample_count)):\n",
    "                sample_timestep_outputs = [t_out[i:i+1] for t_out in timestep_outputs]\n",
    "                \n",
    "                # Analyze confidence evolution\n",
    "                confidences = [F.softmax(t_out, dim=1).max().item() for t_out in sample_timestep_outputs]\n",
    "                for t, conf in enumerate(confidences):\n",
    "                    temporal_analysis['timestep_confidences'][t].append(conf)\n",
    "                \n",
    "                # Prediction stability (how much predictions change across time)\n",
    "                predictions = [t_out.argmax().item() for t_out in sample_timestep_outputs]\n",
    "                stability = len(set(predictions)) / len(predictions)  # Lower = more stable\n",
    "                temporal_analysis['prediction_stability'].append(1.0 - stability)\n",
    "                \n",
    "                # Early vs late accuracy\n",
    "                true_label = target[i].item()\n",
    "                early_preds = predictions[:3]  # First 3 timesteps\n",
    "                late_preds = predictions[-3:]  # Last 3 timesteps\n",
    "                \n",
    "                early_correct = sum(1 for p in early_preds if p == true_label) / len(early_preds)\n",
    "                late_correct = sum(1 for p in late_preds if p == true_label) / len(late_preds)\n",
    "                \n",
    "                temporal_analysis['early_vs_late_accuracy']['early'].append(early_correct)\n",
    "                temporal_analysis['early_vs_late_accuracy']['late'].append(late_correct)\n",
    "                \n",
    "                # Convergence pattern\n",
    "                final_pred = predictions[-1]\n",
    "                convergence_timestep = -1\n",
    "                for t in range(len(predictions)):\n",
    "                    if all(p == final_pred for p in predictions[t:]):\n",
    "                        convergence_timestep = t\n",
    "                        break\n",
    "                temporal_analysis['convergence_patterns'].append(convergence_timestep)\n",
    "                \n",
    "                sample_count += 1\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(f\"\\nüìà Temporal Pattern Analysis Results:\")\n",
    "    print(f\"Average prediction stability: {np.mean(temporal_analysis['prediction_stability']):.3f}\")\n",
    "    \n",
    "    early_acc = np.mean(temporal_analysis['early_vs_late_accuracy']['early'])\n",
    "    late_acc = np.mean(temporal_analysis['early_vs_late_accuracy']['late'])\n",
    "    print(f\"Early accuracy (first 3 steps): {early_acc*100:.2f}%\")\n",
    "    print(f\"Late accuracy (last 3 steps): {late_acc*100:.2f}%\")\n",
    "    print(f\"Improvement over time: {(late_acc - early_acc)*100:.2f}%\")\n",
    "    \n",
    "    # Confidence evolution\n",
    "    print(f\"Confidence evolution across timesteps:\")\n",
    "    for t in range(10):\n",
    "        if temporal_analysis['timestep_confidences'][t]:\n",
    "            avg_conf = np.mean(temporal_analysis['timestep_confidences'][t])\n",
    "            print(f\"   Timestep {t+1}: {avg_conf:.3f}\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    valid_convergence = [c for c in temporal_analysis['convergence_patterns'] if c >= 0]\n",
    "    if valid_convergence:\n",
    "        avg_convergence = np.mean(valid_convergence)\n",
    "        print(f\"Average convergence timestep: {avg_convergence:.1f}\")\n",
    "        print(f\"Samples with convergence: {len(valid_convergence)}/{len(temporal_analysis['convergence_patterns'])} ({len(valid_convergence)/len(temporal_analysis['convergence_patterns'])*100:.1f}%)\")\n",
    "    \n",
    "    return temporal_analysis\n",
    "\n",
    "def save_ttfs_model_state(model_state_dict, filepath, metadata=None):\n",
    "    \"\"\"Save TTFS model state dict with metadata\"\"\"\n",
    "    save_dict = {\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'model_class': 'SpikingCNN_NCARS_TTFS',\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'method': 'IBM TTFS with Identity Mapping and Majority Voting',\n",
    "        'paper': 'High-performance deep spiking neural networks with 0.3 spikes per neuron (Nature Communications 2024)',\n",
    "        'data_format': '[N, C=2, X=120, Y=100, T=10]',\n",
    "        'architecture': '4 conv layers (with initial conv0) + 2 FC layers',\n",
    "    }\n",
    "    if metadata:\n",
    "        save_dict.update(metadata)\n",
    "    \n",
    "    torch.save(save_dict, filepath)\n",
    "\n",
    "def load_ttfs_model_majority(filepath, device='cpu', voting_method='majority', confidence_weighted=False):\n",
    "    \"\"\"Load saved TTFS model with majority voting\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    # Use voting method from checkpoint if available, otherwise use parameter\n",
    "    saved_voting_method = checkpoint.get('voting_method', voting_method)\n",
    "    saved_confidence_weighted = checkpoint.get('confidence_weighted', confidence_weighted)\n",
    "    \n",
    "    model = SpikingCNN_NCARS_TTFS(voting_method=saved_voting_method, \n",
    "                                 confidence_weighted=saved_confidence_weighted)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"üìÇ TTFS model loaded from: {filepath}\")\n",
    "    print(f\"üó≥Ô∏è  Voting method: {saved_voting_method}\")\n",
    "    if saved_confidence_weighted:\n",
    "        print(\"‚öñÔ∏è  Confidence weighting: enabled\")\n",
    "    \n",
    "    if 'model_type' in checkpoint:\n",
    "        if checkpoint['model_type'] == 'best_model':\n",
    "            print(f\"üèÜ This is a best model checkpoint\")\n",
    "            if 'best_accuracy' in checkpoint:\n",
    "                print(f\"   üéØ Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
    "            if 'best_epoch' in checkpoint:\n",
    "                print(f\"   üìç Best epoch: {checkpoint['best_epoch']}\")\n",
    "        elif checkpoint['model_type'] == 'final_model':\n",
    "            print(f\"üîö This is a final model checkpoint\")\n",
    "            if 'final_train_accuracy' in checkpoint:\n",
    "                print(f\"   üéØ Final train accuracy: {checkpoint['final_train_accuracy']:.2f}%\")\n",
    "            if 'final_val_accuracy' in checkpoint and checkpoint['final_val_accuracy']:\n",
    "                print(f\"   üìä Final val accuracy: {checkpoint['final_val_accuracy']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"üìÖ This is an epoch {checkpoint.get('epoch', 'unknown')} checkpoint\")\n",
    "    \n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"   üìç Epoch: {checkpoint['epoch']}\")\n",
    "    if 'train_accuracy' in checkpoint:\n",
    "        print(f\"   üéØ Train accuracy: {checkpoint['train_accuracy']:.2f}%\")\n",
    "    if 'val_accuracy' in checkpoint and checkpoint['val_accuracy']:\n",
    "        print(f\"   üìä Val accuracy: {checkpoint['val_accuracy']:.2f}%\")\n",
    "    if 'voting_statistics' in checkpoint and checkpoint['voting_statistics']:\n",
    "        consensus = checkpoint['voting_statistics'].get('avg_consensus_strength', 'N/A')\n",
    "        print(f\"   üó≥Ô∏è  Average consensus: {consensus}\")\n",
    "    if 'spike_statistics' in checkpoint and checkpoint['spike_statistics']:\n",
    "        stats = checkpoint['spike_statistics']\n",
    "        if 'network_summary' in stats:\n",
    "            avg_spikes = stats['network_summary']['avg_spikes_per_neuron']\n",
    "            print(f\"   ‚ö° Spikes per neuron: {avg_spikes:.3f}\")\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "def list_saved_models(model_save_dir, model_name_prefix='ttfs_snn_majority'):\n",
    "    \"\"\"List all saved models in the directory\"\"\"\n",
    "    if not os.path.exists(model_save_dir):\n",
    "        print(f\"‚ùå Directory {model_save_dir} does not exist\")\n",
    "        return []\n",
    "    \n",
    "    import glob\n",
    "    pattern = os.path.join(model_save_dir, f\"{model_name_prefix}*.pth\")\n",
    "    model_files = glob.glob(pattern)\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"‚ùå No models found with prefix '{model_name_prefix}' in {model_save_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÅ Found {len(model_files)} saved models in {model_save_dir}:\")\n",
    "    \n",
    "    # Sort models by type\n",
    "    best_models = [f for f in model_files if 'best' in f]\n",
    "    final_models = [f for f in model_files if 'final' in f]\n",
    "    epoch_models = [f for f in model_files if 'epoch' in f and 'best' not in f and 'final' not in f]\n",
    "    \n",
    "    if best_models:\n",
    "        print(\"üèÜ Best models:\")\n",
    "        for model_file in sorted(best_models):\n",
    "            print(f\"   {os.path.basename(model_file)}\")\n",
    "    \n",
    "    if final_models:\n",
    "        print(\"üîö Final models:\")\n",
    "        for model_file in sorted(final_models):\n",
    "            print(f\"   {os.path.basename(model_file)}\")\n",
    "    \n",
    "    if epoch_models:\n",
    "        print(\"üìÖ Epoch models:\")\n",
    "        for model_file in sorted(epoch_models):\n",
    "            print(f\"   {os.path.basename(model_file)}\")\n",
    "    \n",
    "    return sorted(model_files)\n",
    "\n",
    "def compare_models(model_files, test_loader, device='cpu'):\n",
    "    \"\"\"Compare multiple saved models on test set\"\"\"\n",
    "    print(f\"\\nüîç Comparing {len(model_files)} models:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        print(f\"\\nüìÇ Testing: {os.path.basename(model_file)}\")\n",
    "        try:\n",
    "            model, checkpoint = load_ttfs_model_majority(model_file, device)\n",
    "            accuracy = test_ncars_ttfs_snn_majority(model, test_loader, device, verbose=False, log_spikes=False)\n",
    "            \n",
    "            result = {\n",
    "                'filename': os.path.basename(model_file),\n",
    "                'filepath': model_file,\n",
    "                'accuracy': accuracy,\n",
    "                'epoch': checkpoint.get('epoch', 'unknown'),\n",
    "                'train_accuracy': checkpoint.get('train_accuracy', 'unknown'),\n",
    "                'val_accuracy': checkpoint.get('val_accuracy', 'unknown'),\n",
    "                'model_type': checkpoint.get('model_type', 'epoch_model'),\n",
    "                'voting_method': checkpoint.get('voting_method', 'unknown')\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   ‚úÖ Test accuracy: {accuracy:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading model: {str(e)}\")\n",
    "    \n",
    "    # Sort by accuracy and display summary\n",
    "    if results:\n",
    "        results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüìä Model Comparison Summary:\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"{'Rank':<4} {'Model':<30} {'Voting':<8} {'Epoch':<6} {'Test Acc':<9} {'Train Acc':<10} {'Val Acc':<9}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            epoch_str = str(result['epoch']) if result['epoch'] != 'unknown' else 'N/A'\n",
    "            train_str = f\"{result['train_accuracy']:.2f}%\" if result['train_accuracy'] != 'unknown' else 'N/A'\n",
    "            val_str = f\"{result['val_accuracy']:.2f}%\" if result['val_accuracy'] != 'unknown' and result['val_accuracy'] is not None else 'N/A'\n",
    "            voting_str = result['voting_method'] if result['voting_method'] != 'unknown' else 'N/A'\n",
    "            \n",
    "            print(f\"{i:<4} {result['filename']:<30} {voting_str:<8} {epoch_str:<6} {result['accuracy']:<9.2f}% {train_str:<10} {val_str:<9}\")\n",
    "        \n",
    "        print(\"=\" * 100)\n",
    "        print(f\"üèÜ Best performing model: {results[0]['filename']} ({results[0]['accuracy']:.2f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_training_history(history, filepath):\n",
    "    \"\"\"Save complete training history including spike statistics\"\"\"\n",
    "    save_dict = {\n",
    "        'training_history': history,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'data_format': '[N, C=2, X=120, Y=100, T=10]',\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, filepath)\n",
    "    print(f\"üìä Training history saved to: {filepath}\")\n",
    "\n",
    "def load_training_history(filepath):\n",
    "    \"\"\"Load training history\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    print(f\"üìà Training history loaded from: {filepath}\")\n",
    "    return checkpoint['training_history']\n",
    "\n",
    "def analyze_spike_patterns(spike_statistics_history):\n",
    "    \"\"\"Analyze spike patterns across training epochs\"\"\"\n",
    "    print(\"\\nüìä Spike Pattern Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not spike_statistics_history:\n",
    "        print(\"No spike statistics available.\")\n",
    "        return\n",
    "    \n",
    "    epochs = [stat['epoch'] for stat in spike_statistics_history]\n",
    "    avg_spikes = [stat['avg_spikes_per_neuron'] for stat in spike_statistics_history]\n",
    "    \n",
    "    print(f\"üìà Spike rate evolution:\")\n",
    "    print(f\"   Initial: {avg_spikes[0]:.3f} spikes/neuron\")\n",
    "    print(f\"   Final: {avg_spikes[-1]:.3f} spikes/neuron\")\n",
    "    print(f\"   Min: {min(avg_spikes):.3f} spikes/neuron\")\n",
    "    print(f\"   Max: {max(avg_spikes):.3f} spikes/neuron\")\n",
    "    \n",
    "    # Analyze per-layer patterns from the last epoch\n",
    "    if spike_statistics_history:\n",
    "        last_detailed = spike_statistics_history[-1]['detailed_stats']\n",
    "        print(f\"\\nüîç Final per-layer analysis:\")\n",
    "        layer_names = ['ttfs0', 'ttfs1', 'ttfs2', 'ttfs3', 'ttfs4', 'ttfs5']\n",
    "        layer_types = ['Conv0', 'Conv1', 'Conv2', 'Conv3', 'FC1', 'FC2']\n",
    "        \n",
    "        for layer_name, layer_type in zip(layer_names, layer_types):\n",
    "            if layer_name in last_detailed:\n",
    "                stats = last_detailed[layer_name]\n",
    "                print(f\"   {layer_type:5} ({layer_name}): {stats['avg_spikes_per_neuron']:.3f} spikes/neuron\")\n",
    "                if 'spikes_per_timestep' in stats and len(stats['spikes_per_timestep']) > 0:\n",
    "                    temporal_pattern = stats['spikes_per_timestep']\n",
    "                    print(f\"         Temporal: {[f'{x:.2f}' for x in temporal_pattern[:5]]}...\")\n",
    "    \n",
    "    # Check if within IBM target range\n",
    "    target_range = 0.3\n",
    "    final_rate = avg_spikes[-1]\n",
    "    if final_rate <= target_range:\n",
    "        print(f\"\\n‚úÖ Target achieved: {final_rate:.3f} ‚â§ {target_range} spikes/neuron\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Above target: {final_rate:.3f} > {target_range} spikes/neuron\")\n",
    "        print(\"   Consider adjusting threshold parameters\")\n",
    "\n",
    "def benchmark_spike_efficiency(model, data_loader, device, num_batches=5):\n",
    "    \"\"\"Benchmark spike efficiency of the TTFS network\"\"\"\n",
    "    print(\"\\n‚ö° Spike Efficiency Benchmark:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    model.eval()\n",
    "    total_spikes = 0\n",
    "    total_neurons = 0\n",
    "    total_timesteps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "                \n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Reset spike statistics\n",
    "            model.reset_spike_statistics()\n",
    "            \n",
    "            # Forward pass\n",
    "            _ = model(data)\n",
    "            \n",
    "            # Collect spike statistics\n",
    "            stats = model.get_network_spike_statistics()\n",
    "            \n",
    "            batch_spikes = stats['network_summary']['total_spikes']\n",
    "            batch_neurons = stats['network_summary']['total_neurons']\n",
    "            \n",
    "            total_spikes += batch_spikes\n",
    "            total_neurons += batch_neurons\n",
    "            total_timesteps += data.shape[-1]  # T dimension\n",
    "            \n",
    "            print(f\"   Batch {batch_idx + 1}: {batch_spikes / batch_neurons:.3f} spikes/neuron\")\n",
    "    \n",
    "    avg_spike_rate = total_spikes / total_neurons if total_neurons > 0 else 0\n",
    "    energy_efficiency = 1.0 / (avg_spike_rate + 1e-8)  # Inverse of spike rate\n",
    "    \n",
    "    print(f\"\\nüìä Efficiency Summary:\")\n",
    "    print(f\"   Average spike rate: {avg_spike_rate:.3f} spikes/neuron\")\n",
    "    print(f\"   Energy efficiency: {energy_efficiency:.1f}x\")\n",
    "    print(f\"   Total spikes: {total_spikes:,}\")\n",
    "    print(f\"   Total neurons: {total_neurons:,}\")\n",
    "    \n",
    "    # Compare with traditional ANN (theoretical)\n",
    "    ann_activations_per_neuron = 1.0  # Each neuron activates once per timestep\n",
    "    efficiency_gain = ann_activations_per_neuron / avg_spike_rate\n",
    "    print(f\"   Efficiency vs ANN: {efficiency_gain:.1f}x reduction in operations\")\n",
    "\n",
    "def create_data_example():\n",
    "    \"\"\"Create example data in the expected format [N, C=2, X=120, Y=100, T=10]\"\"\"\n",
    "    print(\"üìã Creating example data in format [N, C=2, X=120, Y=100, T=10]:\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    batch_size = 8\n",
    "    channels = 2\n",
    "    height = 120\n",
    "    width = 100\n",
    "    timesteps = 10\n",
    "    \n",
    "    # Generate synthetic spatio-temporal data\n",
    "    data = torch.randn(batch_size, channels, height, width, timesteps)\n",
    "    \n",
    "    # Create synthetic labels (binary classification)\n",
    "    labels = torch.randint(0, 2, (batch_size,))\n",
    "    \n",
    "    print(f\"   Data shape: {data.shape}\")\n",
    "    print(f\"   Labels shape: {labels.shape}\")\n",
    "    print(f\"   Data type: {data.dtype}\")\n",
    "    print(f\"   Labels type: {labels.dtype}\")\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Example usage\n",
    "def main_majority_voting():\n",
    "    print(\"üß† TTFS Spiking CNN with Majority Vote Classification (Enhanced)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìÑ Implementation of 'High-performance deep spiking neural networks\")\n",
    "    print(\"   with 0.3 spikes per neuron' (Nature Communications 2024)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéØ Enhanced version with:\")\n",
    "    print(\"   ‚Ä¢ Additional initial convolutional layer (conv0)\")\n",
    "    print(\"   ‚Ä¢ Timestep-wise predictions with majority voting\")\n",
    "    print(\"   ‚Ä¢ Multiple voting strategies (majority, average, weighted)\")\n",
    "    print(\"   ‚Ä¢ Auxiliary timestep loss for better predictions\")\n",
    "    print(\"   ‚Ä¢ Detailed voting and spike analysis\")\n",
    "    print(\"   ‚Ä¢ Temporal pattern analysis\")\n",
    "    print(\"   ‚Ä¢ Model comparison utilities\")\n",
    "    print(\"   ‚Ä¢ Epoch-wise model saving\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üì° Data format: [N, C=2, X=120, Y=100, T=10]\")\n",
    "    print(\"   N: Batch size\")\n",
    "    print(\"   C: 2 channels (e.g., polarity events)\")\n",
    "    print(\"   X: 120 pixels width\")\n",
    "    print(\"   Y: 100 pixels height\")\n",
    "    print(\"   T: 10 temporal frames\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üèóÔ∏è  Architecture: 4 conv layers + 2 FC layers\")\n",
    "    print(\"   ‚Ä¢ Conv0: 2 ‚Üí 32 channels (NEW)\")\n",
    "    print(\"   ‚Ä¢ Conv1: 32 ‚Üí 32 channels\")\n",
    "    print(\"   ‚Ä¢ Conv2: 32 ‚Üí 64 channels\")\n",
    "    print(\"   ‚Ä¢ Conv3: 64 ‚Üí 128 channels\")\n",
    "    print(\"   ‚Ä¢ FC1: 128√ó7√ó6 ‚Üí 256\")\n",
    "    print(\"   ‚Ä¢ FC2: 256 ‚Üí 64\")\n",
    "    print(\"   ‚Ä¢ Output: 64 ‚Üí 2 classes\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create example data\n",
    "    example_data, example_labels = create_data_example()\n",
    "    \n",
    "    print(\"\\nüìù Usage Example:\")\n",
    "    print(\"\"\"\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create your dataset with shape [N, C=2, X=120, Y=100, T=10]\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Train with different voting methods\n",
    "voting_methods = ['majority', 'average', 'weighted']\n",
    "\n",
    "for method in voting_methods:\n",
    "    print(f\"Training with {method} voting...\")\n",
    "    \n",
    "    history = train_ncars_ttfs_snn_majority(\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        num_epochs=20,\n",
    "        voting_method=method,\n",
    "        timestep_loss_weight=0.1,  # Auxiliary loss weight\n",
    "        save_best=True,\n",
    "        save_all_epochs=True,\n",
    "        model_name_prefix=f'ttfs_snn_{method}'\n",
    "    )\n",
    "\n",
    "# List all saved models\n",
    "model_files = list_saved_models('models', 'ttfs_snn_majority')\n",
    "\n",
    "# Compare all saved models\n",
    "results = compare_models(model_files, test_loader, device)\n",
    "\n",
    "# Load and test specific model\n",
    "model, checkpoint = load_ttfs_model_majority('models/ttfs_snn_majority_best.pth', device)\n",
    "test_results = test_ncars_ttfs_snn_majority(model, test_loader, device)\n",
    "\n",
    "# Compare voting methods with same weights\n",
    "best_model_path = 'models/ttfs_snn_majority_best.pth'\n",
    "checkpoint = torch.load(best_model_path)\n",
    "comparison = compare_voting_methods(checkpoint['model_state_dict'], test_loader, device)\n",
    "\n",
    "# Analyze temporal patterns\n",
    "model, _ = load_ttfs_model_majority(best_model_path)\n",
    "temporal_analysis = analyze_temporal_patterns(model, test_loader, device)\n",
    "\n",
    "# Analyze spike patterns\n",
    "analyze_spike_patterns(history['spike_statistics'])\n",
    "\n",
    "# Benchmark efficiency\n",
    "benchmark_spike_efficiency(model, test_loader, device)\n",
    "\n",
    "# Save training history\n",
    "save_training_history(history, 'training_history.pth')\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüéØ Key Features:\")\n",
    "    print(\"   ‚Ä¢ Additional initial conv layer for better feature extraction\")\n",
    "    print(\"   ‚Ä¢ Timestep-wise predictions with final majority voting\")\n",
    "    print(\"   ‚Ä¢ Multiple voting strategies: majority, average, weighted\")\n",
    "    print(\"   ‚Ä¢ Auxiliary timestep loss for better individual predictions\")\n",
    "    print(\"   ‚Ä¢ Detailed voting analysis and consensus tracking\")\n",
    "    print(\"   ‚Ä¢ Temporal pattern analysis across timesteps\")\n",
    "    print(\"   ‚Ä¢ Comparison utilities for different voting methods\")\n",
    "    print(\"   ‚Ä¢ Early vs late prediction accuracy analysis\")\n",
    "    print(\"   ‚Ä¢ Prediction stability and convergence metrics\")\n",
    "    print(\"   ‚Ä¢ Epoch-wise model saving with complete metadata\")\n",
    "    print(\"   ‚Ä¢ Best model tracking and automatic saving\")\n",
    "    print(\"   ‚Ä¢ Spike efficiency benchmarking\")\n",
    "    print(\"   ‚Ä¢ Enhanced depth with 4 convolutional layers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_majority_voting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16335797-78b9-4a8c-8862-a6feca4e01b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Model save directory: models\n",
      "üöÄ Starting TTFS SNN training with majority voting on device: cuda\n",
      "üìä Training samples: 15423\n",
      "üìä Validation samples: 8606\n",
      "üéØ Number of epochs: 20\n",
      "üì° Data format: [N, C=2, X=120, Y=100, T=10]\n",
      "üó≥Ô∏è  Voting method: majority\n",
      "‚öñÔ∏è  Timestep loss weight: 0.1\n",
      "üß† Architecture: 4 conv layers + 2 FC layers (with new initial conv layer)\n",
      "üíæ Best model will be saved to: models\n",
      "üíæ All epoch models will be saved to: models\n",
      "================================================================================\n",
      "üß† TTFS SNN parameters: 1,545,984\n",
      "üéØ Trainable parameters: 1,545,984\n",
      "‚ö° Using Time-to-First-Spike coding with majority voting\n",
      "üî¨ Method: IBM equivalent training (Nature Communications 2024)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  1 | Loss: 1.6955 (Final: 1.6709, TS: 0.2458) | Train Acc:  90.93% | Time: 1027.6s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 191841.585 | Voting consensus: 0.962\n",
      "   ttfs0: 0.716 spikes/neuron\n",
      "   ttfs1: 0.624 spikes/neuron\n",
      "   ttfs2: 0.491 spikes/neuron\n",
      "   ttfs3: 0.500 spikes/neuron\n",
      "   ttfs4: 0.147 spikes/neuron\n",
      "   ttfs5: 0.323 spikes/neuron\n",
      "üîç Running validation... Val Acc:  83.52%\n",
      "üíæ Epoch 1 model saved: models\\ttfs_snn_majority_epoch_01.pth\n",
      "üèÜ New best model saved! Accuracy: 83.52% (Epoch 1)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  2 | Loss: 0.6509 (Final: 0.6389, TS: 0.1194) | Train Acc:  96.53% | Time: 1034.8s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 207819.118 | Voting consensus: 0.980\n",
      "   ttfs0: 0.751 spikes/neuron\n",
      "   ttfs1: 0.654 spikes/neuron\n",
      "   ttfs2: 0.476 spikes/neuron\n",
      "   ttfs3: 0.419 spikes/neuron\n",
      "   ttfs4: 0.129 spikes/neuron\n",
      "   ttfs5: 0.324 spikes/neuron\n",
      "üîç Running validation... Val Acc:  90.00%\n",
      "üíæ Epoch 2 model saved: models\\ttfs_snn_majority_epoch_02.pth\n",
      "üèÜ New best model saved! Accuracy: 90.00% (Epoch 2)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  3 | Loss: 0.5102 (Final: 0.5004, TS: 0.0975) | Train Acc:  97.28% | Time: 1028.1s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 208078.854 | Voting consensus: 0.982\n",
      "   ttfs0: 0.734 spikes/neuron\n",
      "   ttfs1: 0.614 spikes/neuron\n",
      "   ttfs2: 0.439 spikes/neuron\n",
      "   ttfs3: 0.415 spikes/neuron\n",
      "   ttfs4: 0.110 spikes/neuron\n",
      "   ttfs5: 0.325 spikes/neuron\n",
      "üîç Running validation... Val Acc:  90.91%\n",
      "üíæ Epoch 3 model saved: models\\ttfs_snn_majority_epoch_03.pth\n",
      "üèÜ New best model saved! Accuracy: 90.91% (Epoch 3)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  4 | Loss: 0.4750 (Final: 0.4658, TS: 0.0921) | Train Acc:  97.47% | Time: 1043.7s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 208006.638 | Voting consensus: 0.985\n",
      "   ttfs0: 0.772 spikes/neuron\n",
      "   ttfs1: 0.631 spikes/neuron\n",
      "   ttfs2: 0.406 spikes/neuron\n",
      "   ttfs3: 0.330 spikes/neuron\n",
      "   ttfs4: 0.125 spikes/neuron\n",
      "   ttfs5: 0.322 spikes/neuron\n",
      "üîç Running validation... Val Acc:  91.44%\n",
      "üíæ Epoch 4 model saved: models\\ttfs_snn_majority_epoch_04.pth\n",
      "üèÜ New best model saved! Accuracy: 91.44% (Epoch 4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  5 | Loss: 0.3888 (Final: 0.3810, TS: 0.0783) | Train Acc:  97.93% | Time: 1580.6s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 212549.650 | Voting consensus: 0.981\n",
      "   ttfs0: 0.779 spikes/neuron\n",
      "   ttfs1: 0.581 spikes/neuron\n",
      "   ttfs2: 0.374 spikes/neuron\n",
      "   ttfs3: 0.339 spikes/neuron\n",
      "   ttfs4: 0.099 spikes/neuron\n",
      "   ttfs5: 0.317 spikes/neuron\n",
      "üîç Running validation... Val Acc:  92.10%\n",
      "üíæ Epoch 5 model saved: models\\ttfs_snn_majority_epoch_05.pth\n",
      "üèÜ New best model saved! Accuracy: 92.10% (Epoch 5)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  6 | Loss: 0.3547 (Final: 0.3475, TS: 0.0712) | Train Acc:  98.11% | Time: 1001.0s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 209023.702 | Voting consensus: 0.983\n",
      "   ttfs0: 0.762 spikes/neuron\n",
      "   ttfs1: 0.541 spikes/neuron\n",
      "   ttfs2: 0.340 spikes/neuron\n",
      "   ttfs3: 0.270 spikes/neuron\n",
      "   ttfs4: 0.089 spikes/neuron\n",
      "   ttfs5: 0.308 spikes/neuron\n",
      "üîç Running validation... Val Acc:  87.00%\n",
      "üíæ Epoch 6 model saved: models\\ttfs_snn_majority_epoch_06.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  7 | Loss: 0.3195 (Final: 0.3129, TS: 0.0663) | Train Acc:  98.30% | Time: 963.3s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 203694.474 | Voting consensus: 0.985\n",
      "   ttfs0: 0.759 spikes/neuron\n",
      "   ttfs1: 0.517 spikes/neuron\n",
      "   ttfs2: 0.331 spikes/neuron\n",
      "   ttfs3: 0.216 spikes/neuron\n",
      "   ttfs4: 0.086 spikes/neuron\n",
      "   ttfs5: 0.330 spikes/neuron\n",
      "üîç Running validation... Val Acc:  89.69%\n",
      "üíæ Epoch 7 model saved: models\\ttfs_snn_majority_epoch_07.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  8 | Loss: 0.3035 (Final: 0.2974, TS: 0.0608) | Train Acc:  98.39% | Time: 955.0s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 199835.136 | Voting consensus: 0.989\n",
      "   ttfs0: 0.773 spikes/neuron\n",
      "   ttfs1: 0.527 spikes/neuron\n",
      "   ttfs2: 0.303 spikes/neuron\n",
      "   ttfs3: 0.209 spikes/neuron\n",
      "   ttfs4: 0.105 spikes/neuron\n",
      "   ttfs5: 0.300 spikes/neuron\n",
      "üîç Running validation... Val Acc:  92.67%\n",
      "üíæ Epoch 8 model saved: models\\ttfs_snn_majority_epoch_08.pth\n",
      "üèÜ New best model saved! Accuracy: 92.67% (Epoch 8)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch  9 | Loss: 0.2663 (Final: 0.2604, TS: 0.0590) | Train Acc:  98.59% | Time: 955.2s | LR: 1.00e-03\n",
      "‚ö° Avg spikes/neuron: 196734.618 | Voting consensus: 0.987\n",
      "   ttfs0: 0.735 spikes/neuron\n",
      "   ttfs1: 0.445 spikes/neuron\n",
      "   ttfs2: 0.273 spikes/neuron\n",
      "   ttfs3: 0.186 spikes/neuron\n",
      "   ttfs4: 0.068 spikes/neuron\n",
      "   ttfs5: 0.277 spikes/neuron\n",
      "üîç Running validation... Val Acc:  92.31%\n",
      "üíæ Epoch 9 model saved: models\\ttfs_snn_majority_epoch_09.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 10 | Loss: 0.2527 (Final: 0.2473, TS: 0.0549) | Train Acc:  98.66% | Time: 949.4s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 190657.348 | Voting consensus: 0.988\n",
      "   ttfs0: 0.753 spikes/neuron\n",
      "   ttfs1: 0.394 spikes/neuron\n",
      "   ttfs2: 0.219 spikes/neuron\n",
      "   ttfs3: 0.136 spikes/neuron\n",
      "   ttfs4: 0.087 spikes/neuron\n",
      "   ttfs5: 0.302 spikes/neuron\n",
      "üîç Running validation... Val Acc:  88.83%\n",
      "üíæ Epoch 10 model saved: models\\ttfs_snn_majority_epoch_10.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 11 | Loss: 0.1360 (Final: 0.1326, TS: 0.0340) | Train Acc:  99.28% | Time: 952.1s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 188628.117 | Voting consensus: 0.989\n",
      "   ttfs0: 0.743 spikes/neuron\n",
      "   ttfs1: 0.414 spikes/neuron\n",
      "   ttfs2: 0.210 spikes/neuron\n",
      "   ttfs3: 0.140 spikes/neuron\n",
      "   ttfs4: 0.093 spikes/neuron\n",
      "   ttfs5: 0.324 spikes/neuron\n",
      "üîç Running validation... Val Acc:  92.91%\n",
      "üíæ Epoch 11 model saved: models\\ttfs_snn_majority_epoch_11.pth\n",
      "üèÜ New best model saved! Accuracy: 92.91% (Epoch 11)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 12 | Loss: 0.1142 (Final: 0.1111, TS: 0.0308) | Train Acc:  99.40% | Time: 952.8s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 187639.104 | Voting consensus: 0.991\n",
      "   ttfs0: 0.757 spikes/neuron\n",
      "   ttfs1: 0.366 spikes/neuron\n",
      "   ttfs2: 0.186 spikes/neuron\n",
      "   ttfs3: 0.121 spikes/neuron\n",
      "   ttfs4: 0.100 spikes/neuron\n",
      "   ttfs5: 0.356 spikes/neuron\n",
      "üîç Running validation... Val Acc:  91.75%\n",
      "üíæ Epoch 12 model saved: models\\ttfs_snn_majority_epoch_12.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 13 | Loss: 0.0936 (Final: 0.0908, TS: 0.0285) | Train Acc:  99.51% | Time: 951.3s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 187706.167 | Voting consensus: 0.993\n",
      "   ttfs0: 0.747 spikes/neuron\n",
      "   ttfs1: 0.403 spikes/neuron\n",
      "   ttfs2: 0.212 spikes/neuron\n",
      "   ttfs3: 0.136 spikes/neuron\n",
      "   ttfs4: 0.105 spikes/neuron\n",
      "   ttfs5: 0.362 spikes/neuron\n",
      "üîç Running validation... Val Acc:  91.72%\n",
      "üíæ Epoch 13 model saved: models\\ttfs_snn_majority_epoch_13.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 14 | Loss: 0.1118 (Final: 0.1088, TS: 0.0301) | Train Acc:  99.41% | Time: 955.1s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 186634.714 | Voting consensus: 0.993\n",
      "   ttfs0: 0.758 spikes/neuron\n",
      "   ttfs1: 0.334 spikes/neuron\n",
      "   ttfs2: 0.152 spikes/neuron\n",
      "   ttfs3: 0.096 spikes/neuron\n",
      "   ttfs4: 0.095 spikes/neuron\n",
      "   ttfs5: 0.395 spikes/neuron\n",
      "üîç Running validation... Val Acc:  87.06%\n",
      "üíæ Epoch 14 model saved: models\\ttfs_snn_majority_epoch_14.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 15 | Loss: 0.1104 (Final: 0.1075, TS: 0.0293) | Train Acc:  99.42% | Time: 954.1s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 184506.382 | Voting consensus: 0.991\n",
      "   ttfs0: 0.759 spikes/neuron\n",
      "   ttfs1: 0.336 spikes/neuron\n",
      "   ttfs2: 0.139 spikes/neuron\n",
      "   ttfs3: 0.102 spikes/neuron\n",
      "   ttfs4: 0.096 spikes/neuron\n",
      "   ttfs5: 0.362 spikes/neuron\n",
      "üîç Running validation... Val Acc:  93.07%\n",
      "üíæ Epoch 15 model saved: models\\ttfs_snn_majority_epoch_15.pth\n",
      "üèÜ New best model saved! Accuracy: 93.07% (Epoch 15)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 16 | Loss: 0.0947 (Final: 0.0920, TS: 0.0277) | Train Acc:  99.50% | Time: 969.6s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 183772.289 | Voting consensus: 0.993\n",
      "   ttfs0: 0.743 spikes/neuron\n",
      "   ttfs1: 0.350 spikes/neuron\n",
      "   ttfs2: 0.173 spikes/neuron\n",
      "   ttfs3: 0.113 spikes/neuron\n",
      "   ttfs4: 0.083 spikes/neuron\n",
      "   ttfs5: 0.317 spikes/neuron\n",
      "üîç Running validation... Val Acc:  93.62%\n",
      "üíæ Epoch 16 model saved: models\\ttfs_snn_majority_epoch_16.pth\n",
      "üèÜ New best model saved! Accuracy: 93.62% (Epoch 16)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 17 | Loss: 0.0848 (Final: 0.0824, TS: 0.0244) | Train Acc:  99.55% | Time: 965.9s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 182858.075 | Voting consensus: 0.993\n",
      "   ttfs0: 0.759 spikes/neuron\n",
      "   ttfs1: 0.327 spikes/neuron\n",
      "   ttfs2: 0.136 spikes/neuron\n",
      "   ttfs3: 0.083 spikes/neuron\n",
      "   ttfs4: 0.111 spikes/neuron\n",
      "   ttfs5: 0.293 spikes/neuron\n",
      "üîç Running validation... Val Acc:  93.82%\n",
      "üíæ Epoch 17 model saved: models\\ttfs_snn_majority_epoch_17.pth\n",
      "üèÜ New best model saved! Accuracy: 93.82% (Epoch 17)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 18 | Loss: 0.0946 (Final: 0.0920, TS: 0.0264) | Train Acc:  99.50% | Time: 970.8s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 182735.985 | Voting consensus: 0.994\n",
      "   ttfs0: 0.742 spikes/neuron\n",
      "   ttfs1: 0.342 spikes/neuron\n",
      "   ttfs2: 0.135 spikes/neuron\n",
      "   ttfs3: 0.101 spikes/neuron\n",
      "   ttfs4: 0.096 spikes/neuron\n",
      "   ttfs5: 0.293 spikes/neuron\n",
      "üîç Running validation... Val Acc:  90.84%\n",
      "üíæ Epoch 18 model saved: models\\ttfs_snn_majority_epoch_18.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 19 | Loss: 0.0679 (Final: 0.0657, TS: 0.0221) | Train Acc:  99.64% | Time: 960.2s | LR: 5.00e-04\n",
      "‚ö° Avg spikes/neuron: 175261.817 | Voting consensus: 0.991\n",
      "   ttfs0: 0.718 spikes/neuron\n",
      "   ttfs1: 0.289 spikes/neuron\n",
      "   ttfs2: 0.138 spikes/neuron\n",
      "   ttfs3: 0.082 spikes/neuron\n",
      "   ttfs4: 0.077 spikes/neuron\n",
      "   ttfs5: 0.326 spikes/neuron\n",
      "üîç Running validation... Val Acc:  93.12%\n",
      "üíæ Epoch 19 model saved: models\\ttfs_snn_majority_epoch_19.pth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 20 | Loss: 0.0715 (Final: 0.0693, TS: 0.0223) | Train Acc:  99.62% | Time: 870.4s | LR: 2.50e-04\n",
      "‚ö° Avg spikes/neuron: 174101.982 | Voting consensus: 0.995\n",
      "   ttfs0: 0.720 spikes/neuron\n",
      "   ttfs1: 0.298 spikes/neuron\n",
      "   ttfs2: 0.122 spikes/neuron\n",
      "   ttfs3: 0.078 spikes/neuron\n",
      "   ttfs4: 0.072 spikes/neuron\n",
      "   ttfs5: 0.399 spikes/neuron\n",
      "üîç Running validation... Val Acc:  90.50%\n",
      "üíæ Epoch 20 model saved: models\\ttfs_snn_majority_epoch_20.pth\n",
      "--------------------------------------------------------------------------------\n",
      "üîö Final model saved: models\\ttfs_snn_majority_final.pth\n",
      "üéâ TTFS SNN majority voting training completed!\n",
      "‚è±Ô∏è  Total training time: 503.9 minutes\n",
      "üìà Best training accuracy: 99.64%\n",
      "üìä Best validation accuracy: 93.82%\n",
      "\n",
      "üíæ Saved Models Summary:\n",
      "   üìÅ Epoch models: models/ttfs_snn_majority_epoch_XX.pth\n",
      "   üìÅ Final model: models/ttfs_snn_majority_final.pth\n",
      "   üèÜ Best model: models/ttfs_snn_majority_best.pth\n",
      "       Best accuracy: 93.82% at epoch 17\n",
      "‚ö° Final average spikes per neuron: 174101.982\n"
     ]
    }
   ],
   "source": [
    "method = 'majority'\n",
    "\n",
    "history = train_ncars_ttfs_snn_majority(\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        num_epochs=20,\n",
    "        voting_method=method,\n",
    "        timestep_loss_weight=0.1,  # Auxiliary loss weight\n",
    "        save_best=True,\n",
    "        save_all_epochs=True,\n",
    "        model_name_prefix=f'ttfs_snn_{method}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f858fa3-e794-4634-838f-965f267e3ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training history saved to: training_history.pth\n"
     ]
    }
   ],
   "source": [
    "save_training_history(history, 'training_history.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2873a-aa8e-4417-9bfb-b1bbfcad9ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
